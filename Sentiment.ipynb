{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "43f39934-760e-423c-85a0-ddbd4d3d5e09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'stopwords' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [42]\u001b[0m, in \u001b[0;36m<cell line: 93>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     90\u001b[0m     showData()\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 94\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [42]\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m     79\u001b[0m     text\u001b[38;5;241m.\u001b[39mappend(i[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m---> 81\u001b[0m stopwords \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[43mstopwords\u001b[49m\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     82\u001b[0m stopwords\u001b[38;5;241m.\u001b[39mupdate([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRT\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCO\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m//\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m@\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     84\u001b[0m comments \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin( comments \u001b[38;5;28;01mfor\u001b[39;00m comments \u001b[38;5;129;01min\u001b[39;00m text)\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'stopwords' referenced before assignment"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from os import path\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import requests\n",
    "import json\n",
    "\n",
    "#Input: \n",
    "username = \"lexfridman\"\n",
    "\n",
    "#Dependencies\n",
    "APIKEY = \"M4WV85v7m2xFDVownYHXxZMTI\"\n",
    "API_SECRET = \"BmaQmu4Tw0b2IFnHW9Vs43NZ8iqwIdrQpNypPp67KnEiJ9ih11\"\n",
    "BEARER = \"AAAAAAAAAAAAAAAAAAAAAJEddAEAAAAAEzFi3yt567TOL1uM%2BeM7mFPgZwg%3Dl23TFu1cLN3DabEJlgulduCSEil2f0ax7FLM1N147TH21Xk4kO\"\n",
    "\n",
    "def get_url():\n",
    "    usernames = f\"usernames={username}\"\n",
    "    user_fields = \"user.fields=id\"\n",
    "    url = \"https://api.twitter.com/2/users/by?{}&{}\".format(usernames, user_fields)\n",
    "    return url\n",
    "\n",
    "def getUserId():\n",
    "    url = get_url()\n",
    "    response = requests.request(\"GET\", url, auth=bearer_oauth,)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    result = response.json()\n",
    "    return result[\"data\"][0][\"id\"]\n",
    "\n",
    "def bearer_oauth(r):\n",
    "    \"\"\"\n",
    "    Method required by bearer token authentication.\n",
    "    \"\"\"\n",
    "    r.headers[\"Authorization\"] = f\"Bearer {BEARER}\"\n",
    "    r.headers[\"User-Agent\"] = \"v2RecentSearchPython\"\n",
    "    return r\n",
    "\n",
    "def getAllTweets(id):\n",
    "    url = \"https://api.twitter.com/2/users/{}/tweets\".format(id)\n",
    "    param = {\"max_results\":100}\n",
    "    response = requests.get(url, auth=bearer_oauth,params=param)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    result = response.json()\n",
    "    return result    \n",
    "\n",
    "def showData():\n",
    "    an = SentimentIntensityAnalyzer()\n",
    "    neg, pos, neu, compound = 0,0,0,0\n",
    "    for line in text:\n",
    "        res = an.polarity_scores(line)\n",
    "        neg += res[\"neg\"]\n",
    "        pos += res[\"pos\"]\n",
    "        neu += res[\"neu\"]\n",
    "        compound += res[\"compound\"]\n",
    "    avg_neg = round( neg/len(text) * 100, 2)\n",
    "    avg_pos = round( pos/len(text) * 100, 2)\n",
    "    avg_neu = round ( neu/len(text) * 100, 2)\n",
    "    avg_comp = round( compound/len(text) * 100, 2)\n",
    "\n",
    "    print(\"Neg average : \" + str( avg_neg ) )\n",
    "    print(\"Pos average : \" + str( avg_pos ) )\n",
    "    print(\"Neu average : \" + str( avg_neu ) )\n",
    "    print(\"Compound average : \" + str( avg_comp ) )\n",
    "\n",
    "\n",
    "def main():\n",
    "    userId = getUserId()\n",
    "    data = getAllTweets(userId)\n",
    "    text = []\n",
    "    for i in data[\"data\"]:\n",
    "        text.append(i[\"text\"])\n",
    "    \n",
    "    stopwords = set(stopwords.words(\"english\"))\n",
    "    stopwords.update([\"br\", \"href\", \"https\", \"RT\", \"CO\", \"//\", \"@\"])\n",
    "\n",
    "    comments = \" \".join( comments for comments in text)\n",
    "    cloud = WordCloud(stopwords=stopwords).generate(comments)\n",
    "\n",
    "    plt.imshow(cloud,interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    showData()\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e1a9c2-e76a-4863-8e2b-4e91430102a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69439e96-c464-461f-b6b4-c895a6099d3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
